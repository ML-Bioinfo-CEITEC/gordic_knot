{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849e7c9b-4008-4805-ac62-315aa1a61dec",
   "metadata": {},
   "source": [
    "# Flip knot <-> unknot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aef1cf3-5e5c-4c43-a07f-03e23d1d2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from topoly import alexander\n",
    "\n",
    "from foldingdiff import datasets\n",
    "from foldingdiff.angles_and_coords import create_new_chain_nerf\n",
    "from foldingdiff import utils\n",
    "from foldingdiff import modelling\n",
    "\n",
    "from GPyOpt.methods import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4fa499-1db6-4cdc-9beb-a07045bea644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2bf819f290>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3017aae4-f4b1-4a1a-97bc-539bc1fadfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"wukevin/foldingdiff_cath\"\n",
    "BATCH_SIZE = 512\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "LR = 5e-4\n",
    "EPOCHS = 40\n",
    "ITERATION_STEPS = 500\n",
    "FLIPPING_LR = 0.001"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8ef6684-364f-40f9-a10a-2738710c0d4d",
   "metadata": {},
   "source": [
    "diffusion_model = modelling.BertForDiffusion.from_dir(snapshot_download(MODEL)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e6d38-910c-435e-a7ef-74b0191de20a",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "744f1a1e-ebdb-44cc-b848-dcf9b488b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(datasets.CathCanonicalAnglesOnlyDataset):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.labels = list(map(lambda x: 0 if x.split('/')[1][0] == 'u' else 1, self.filenames))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return_dict = super().__getitem__(index)\n",
    "        label = self.labels[index]\n",
    "        return return_dict, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8238240b-87c2-47f8-a9ab-5b1426c43a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = myDataset(\n",
    "            pdbs='mixed_dataset',\n",
    "            split=None,\n",
    "            pad=128,\n",
    "            min_length=40,\n",
    "            trim_strategy=\"randomcrop\",\n",
    "            zero_center=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fa8e724-0c5b-418d-b0a9-a361eedd1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "valid_size = int(0.1 * len(dataset))  # 10% for validation\n",
    "test_size = len(dataset) - train_size - valid_size # 10% for testing\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=32)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=32)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71ed79-2e59-4f05-8801-8abdeda0b30c",
   "metadata": {},
   "source": [
    "### Check the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af938db-68db-494f-a74f-47272d2dc47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([512, 128, 6])\n",
      "Labels batch shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_data_loader))\n",
    "print(f\"Feature batch shape: {train_features['angles'].size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792fac8f-b18e-48e3-b808-d6b7717e20f9",
   "metadata": {},
   "source": [
    "## Make simple classifier using foldingdiff representation as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebad966b-ea82-44f8-8909-a0b0704f38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):  \n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool1d(64)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(384, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(384, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_representation):\n",
    "        \n",
    "        # Pool the input\n",
    "        pooled_output = self.pooling(input_representation.transpose(1, 2))\n",
    "        \n",
    "        # Flatten the pooled output\n",
    "        flattened_output = pooled_output.reshape(pooled_output.size(0), -1)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(flattened_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d3346d4-c258-4aad-892e-95afe6a3e9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        labels = labels.float().to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs['angles'].to(DEVICE))\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb2f9d12-d5be-469d-a55f-e30b2a85ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels = batch\n",
    "            labels = labels.float().to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs['angles'].to(DEVICE))\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = (outputs.squeeze() > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e7c29-994f-42f3-b695-0b03cb608fbe",
   "metadata": {},
   "source": [
    "## Include Bayesian optimization for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5157e5db-91f0-4122-bea9-b3c1638ece23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_for_optimization(params):\n",
    "    learning_rate = params[0][0]\n",
    "    num_epochs = int(params[0][1])\n",
    "    \n",
    "    simple_model = SimpleClassifier().to(DEVICE)\n",
    "    \n",
    "    adam = optim.AdamW(simple_model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    print(f\"Learning rate: {learning_rate}, Epochs: {num_epochs}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_model(simple_model, train_data_loader, adam, criterion)\n",
    "    \n",
    "    avg_loss, accuracy = evaluate_model(simple_model, valid_data_loader, criterion)\n",
    "\n",
    "    print(f'Loss: {avg_loss}, Accuracy: {accuracy}\\n')\n",
    "    \n",
    "    return 1 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3eaec14-997b-4b93-b72f-d06eb6cc41af",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_space = [\n",
    "    {'name': 'lr', 'type': 'continuous', 'domain': (5e-6, 5e-2)},\n",
    "    {'name': 'epochs', 'type': 'continuous', 'domain': (50, 300)}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53b66a65-4aae-44ed-9cb0-5b8c974aa181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.03802280052487008, Epochs: 110\n",
      "Loss: 2.310053586959839, Accuracy: 0.8058252427184466\n",
      "\n",
      "Learning rate: 0.02443337646252407, Epochs: 112\n",
      "Loss: 1.4338037967681885, Accuracy: 0.8252427184466019\n",
      "\n",
      "Learning rate: 0.03145738976359821, Epochs: 221\n",
      "Loss: 1.3800090551376343, Accuracy: 0.8349514563106796\n",
      "\n",
      "Learning rate: 0.009474440598891633, Epochs: 251\n",
      "Loss: 1.1808702945709229, Accuracy: 0.8543689320388349\n",
      "\n",
      "Learning rate: 0.02039576845329432, Epochs: 91\n",
      "Loss: 1.4331005811691284, Accuracy: 0.8252427184466019\n",
      "\n",
      "Learning rate: 0.03368800597087874, Epochs: 250\n",
      "Loss: 1.3053553104400635, Accuracy: 0.8252427184466019\n",
      "\n",
      "Learning rate: 0.008944459807988619, Epochs: 251\n",
      "Loss: 1.1283518075942993, Accuracy: 0.8446601941747572\n",
      "\n",
      "Learning rate: 0.0203827049580371, Epochs: 250\n",
      "Loss: 1.2729368209838867, Accuracy: 0.8349514563106796\n",
      "\n",
      "Learning rate: 5e-06, Epochs: 251\n",
      "Loss: 0.6698043942451477, Accuracy: 0.6116504854368932\n",
      "\n",
      "Learning rate: 0.04487936979552982, Epochs: 251\n",
      "Loss: 1.365583896636963, Accuracy: 0.8252427184466019\n",
      "\n",
      "Learning rate: 0.023542410415050997, Epochs: 91\n",
      "Loss: 1.3241673707962036, Accuracy: 0.8349514563106796\n",
      "\n",
      "Learning rate: 0.02796581077415081, Epochs: 91\n",
      "Loss: 1.1500035524368286, Accuracy: 0.8252427184466019\n",
      "\n",
      "Learning rate: 0.0034846005985501634, Epochs: 91\n",
      "Loss: 0.8113376498222351, Accuracy: 0.8446601941747572\n",
      "\n",
      "Learning rate: 0.03883248366512395, Epochs: 91\n",
      "Loss: 1.2613537311553955, Accuracy: 0.8252427184466019\n",
      "\n",
      "Learning rate: 0.002125865547315336, Epochs: 91\n",
      "Loss: 0.6876181364059448, Accuracy: 0.8446601941747572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=evaluation_for_optimization, domain=optimization_space, model_type='GP',\n",
    "    acquisition_type='EI', max_iter=10\n",
    ")\n",
    "optimizer.run_optimization(max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d986c0-b427-437a-8aa4-7aadb959b120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "Learning rate: 0.009474440598891633\n",
      "Number of epochs: 251\n",
      "Best objective value: 0.1456310679611651\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters:\")\n",
    "print(f\"Learning rate: {optimizer.X[np.argmin(optimizer.Y), 0]}\")\n",
    "print(f\"Number of epochs: {int(optimizer.X[np.argmin(optimizer.Y), 1])}\")\n",
    "print(f\"Best objective value: {np.min(optimizer.Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c0361a-3787-4470-ba6d-8d8060b72dc0",
   "metadata": {},
   "source": [
    "### Get model with the optimal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b83c564b-81a1-4b41-a8ef-4ee77d76bda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.009474440598891633, Epochs: 251\n",
      "Loss: 1.218287706375122, Accuracy: 0.8543689320388349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = optimizer.X[np.argmin(optimizer.Y), 0]\n",
    "num_epochs = int(optimizer.X[np.argmin(optimizer.Y), 1])\n",
    "\n",
    "simple_model = SimpleClassifier().to(DEVICE)\n",
    "    \n",
    "adam = optim.AdamW(simple_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "    \n",
    "print(f\"Learning rate: {learning_rate}, Epochs: {num_epochs}\")\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    train_model(simple_model, train_data_loader, adam, criterion)\n",
    "    \n",
    "avg_loss, accuracy = evaluate_model(simple_model, valid_data_loader, criterion)\n",
    "\n",
    "print(f'Loss: {avg_loss}, Accuracy: {accuracy}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036422f4-d88d-4035-98df-324e632b8134",
   "metadata": {},
   "source": [
    "## Adversarial training - flip structures from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c0e1b7-c4af-4694-b61f-430d1b2b19cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleClassifier(\n",
       "  (pooling): AdaptiveAvgPool1d(output_size=64)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=384, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a717c953-b281-4368-b3b1-b78973e2986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative process to modify the input\n",
    "def guide_structure(input_structure, target_label, verbose=False):\n",
    "    for iteration in range(ITERATION_STEPS):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = simple_model(input_structure)\n",
    "\n",
    "        # Compute the loss (maximize the prediction towards the target label)\n",
    "        loss = loss_fn(output, target_label)\n",
    "\n",
    "        # Backward pass and update the input structure\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # TODO: Apply constraints to keep the structure valid?\n",
    "        with torch.no_grad():\n",
    "            angular_idx = np.where(dataset.feature_is_angular['angles'])[0]\n",
    "            # Copy the input structure to avoid in-place modification\n",
    "            modified_structure = input_structure.clone()\n",
    "            for s in modified_structure:\n",
    "                s[..., angular_idx] = utils.modulo_with_wrapped_range(\n",
    "                    s[..., angular_idx], range_min=-np.pi, range_max=np.pi\n",
    "                )\n",
    "            # Update the input structure with the modified one\n",
    "            input_structure.copy_(modified_structure)\n",
    "        \n",
    "        # Print the progress\n",
    "        if iteration % 10 == 0 and verbose:\n",
    "            print(f\"Iteration {iteration}: Loss = {loss.item()}, Prediction = {output.item()}\")\n",
    "            \n",
    "    return input_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61c84911-aa66-4e20-9ea8-7ccb5aec0e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_to_structure(sample, length, index=0):\n",
    "    retval = [s + dataset.get_masked_means() for s in sample]\n",
    "    # Because shifting may have caused us to go across the circle boundary, re-wrap\n",
    "    angular_idx = np.where(dataset.feature_is_angular['angles'])[0]\n",
    "    for s in retval:\n",
    "        s[..., angular_idx] = utils.modulo_with_wrapped_range(\n",
    "                    s[..., angular_idx], range_min=-np.pi, range_max=np.pi\n",
    "                )\n",
    "    df = pd.DataFrame(retval, columns=['phi', 'psi', 'omega', 'tau', 'CA:C:1N', 'C:1N:1CA']).astype(\"float\")[:int(length)]\n",
    "\n",
    "    create_new_chain_nerf(f'tmp/flip_{index}.pdb', df)\n",
    "    \n",
    "    knots = alexander(f'tmp/flip_{index}.pdb', tries=100)\n",
    "    \n",
    "    print('Knotting status: ', knots)\n",
    "    \n",
    "    if ('0_1' in knots.keys() and knots['0_1'] > 0.5):\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "502509b4-85ba-41f9-ac21-71305125b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = next(iter(test_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44b93d8c-f59c-4b66-83d6-f48148614004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ---- Attempt 0 ----- \n",
      "\n",
      "Knotting status:  {'0_1': 0.99}\n",
      "Iteration 0: Loss = 2.6374919414520264, Prediction = 0.0715404748916626\n",
      "Iteration 10: Loss = 0.12626273930072784, Prediction = 0.8813832402229309\n",
      "Iteration 20: Loss = 0.01303718239068985, Prediction = 0.9870474338531494\n",
      "Iteration 30: Loss = 0.004926578141748905, Prediction = 0.9950855374336243\n",
      "Iteration 40: Loss = 0.003274909919127822, Prediction = 0.9967304468154907\n",
      "Iteration 50: Loss = 0.002712288871407509, Prediction = 0.9972913861274719\n",
      "Iteration 60: Loss = 0.0024566403590142727, Prediction = 0.997546374797821\n",
      "Iteration 70: Loss = 0.002305122558027506, Prediction = 0.9976975321769714\n",
      "Iteration 80: Loss = 0.002192096784710884, Prediction = 0.9978103041648865\n",
      "Iteration 90: Loss = 0.0020945535507053137, Prediction = 0.9979076385498047\n",
      "Iteration 100: Loss = 0.0020046643912792206, Prediction = 0.9979973435401917\n",
      "Iteration 110: Loss = 0.001919322181493044, Prediction = 0.9980825185775757\n",
      "Iteration 120: Loss = 0.0018379281973466277, Prediction = 0.9981637597084045\n",
      "Iteration 130: Loss = 0.0017601235304027796, Prediction = 0.9982414245605469\n",
      "Iteration 140: Loss = 0.0016863251803442836, Prediction = 0.9983150959014893\n",
      "Iteration 150: Loss = 0.001616950030438602, Prediction = 0.9983843564987183\n",
      "Iteration 160: Loss = 0.0015511616365984082, Prediction = 0.9984500408172607\n",
      "Iteration 170: Loss = 0.0014884218107908964, Prediction = 0.9985126852989197\n",
      "Iteration 180: Loss = 0.001428909250535071, Prediction = 0.9985721111297607\n",
      "Iteration 190: Loss = 0.0013724442105740309, Prediction = 0.9986284971237183\n",
      "Iteration 200: Loss = 0.0013187875738367438, Prediction = 0.9986820816993713\n",
      "Iteration 210: Loss = 0.0012678193161264062, Prediction = 0.9987329840660095\n",
      "Iteration 220: Loss = 0.001219479483552277, Prediction = 0.9987812638282776\n",
      "Iteration 230: Loss = 0.0011736482847481966, Prediction = 0.9988270401954651\n",
      "Iteration 240: Loss = 0.001130086719058454, Prediction = 0.9988705515861511\n",
      "Iteration 250: Loss = 0.0010883768554776907, Prediction = 0.9989122152328491\n",
      "Iteration 260: Loss = 0.001048876321874559, Prediction = 0.9989516735076904\n",
      "Iteration 270: Loss = 0.001011227024719119, Prediction = 0.9989892840385437\n",
      "Iteration 280: Loss = 0.0009754883940331638, Prediction = 0.9990249872207642\n",
      "Iteration 290: Loss = 0.0009415408712811768, Prediction = 0.9990589022636414\n",
      "Iteration 300: Loss = 0.0009092648979276419, Prediction = 0.9990911483764648\n",
      "Iteration 310: Loss = 0.0008786603575572371, Prediction = 0.9991217255592346\n",
      "Iteration 320: Loss = 0.0008494884823448956, Prediction = 0.9991508722305298\n",
      "Iteration 330: Loss = 0.0008216298301704228, Prediction = 0.9991787075996399\n",
      "Iteration 340: Loss = 0.0007951440056785941, Prediction = 0.9992051720619202\n",
      "Iteration 350: Loss = 0.0007699114503338933, Prediction = 0.9992303848266602\n",
      "Iteration 360: Loss = 0.0007459918851964176, Prediction = 0.9992542862892151\n",
      "Iteration 370: Loss = 0.0007229676120914519, Prediction = 0.9992772936820984\n",
      "Iteration 380: Loss = 0.0007010771078057587, Prediction = 0.999299168586731\n",
      "Iteration 390: Loss = 0.0006800817791372538, Prediction = 0.9993201494216919\n",
      "Iteration 400: Loss = 0.0006602201028726995, Prediction = 0.9993399977684021\n",
      "Iteration 410: Loss = 0.0006410149508155882, Prediction = 0.9993591904640198\n",
      "Iteration 420: Loss = 0.0006226451369002461, Prediction = 0.9993775486946106\n",
      "Iteration 430: Loss = 0.000605408800765872, Prediction = 0.9993947744369507\n",
      "Iteration 440: Loss = 0.0005891866167075932, Prediction = 0.9994109869003296\n",
      "Iteration 450: Loss = 0.00057368038687855, Prediction = 0.9994264841079712\n",
      "Iteration 460: Loss = 0.0005588304484263062, Prediction = 0.9994413256645203\n",
      "Iteration 470: Loss = 0.0005445174174383283, Prediction = 0.9994556307792664\n",
      "Iteration 480: Loss = 0.0005306817474775016, Prediction = 0.9994694590568542\n",
      "Iteration 490: Loss = 0.0005175021942704916, Prediction = 0.9994826316833496\n",
      "Knotting status:  {'0_1': 1.0}\n",
      "\n",
      "\n",
      " ---- Attempt 1 ----- \n",
      "\n",
      "Knotting status:  {'3_1': 0.99}\n",
      "Iteration 0: Loss = 10.397207260131836, Prediction = 0.999969482421875\n",
      "Iteration 10: Loss = 11.693889617919922, Prediction = 0.9999916553497314\n",
      "Iteration 20: Loss = 6.708181858062744, Prediction = 0.9987791180610657\n",
      "Iteration 30: Loss = 0.9189863204956055, Prediction = 0.6010767817497253\n",
      "Iteration 40: Loss = 0.0070042358711361885, Prediction = 0.006979783531278372\n",
      "Iteration 50: Loss = 0.0006289075245149434, Prediction = 0.0006287132855504751\n",
      "Iteration 60: Loss = 0.0002317101025255397, Prediction = 0.00023166718892753124\n",
      "Iteration 70: Loss = 0.0001574878697283566, Prediction = 0.00015745186829008162\n",
      "Iteration 80: Loss = 0.00013590783055406064, Prediction = 0.00013590084563475102\n",
      "Iteration 90: Loss = 0.00012839664123021066, Prediction = 0.00012840967974625528\n",
      "Iteration 100: Loss = 0.0001255352544831112, Prediction = 0.00012550388055387884\n",
      "Iteration 110: Loss = 0.0001242237922269851, Prediction = 0.00012420806160662323\n",
      "Iteration 120: Loss = 0.00012350844917818904, Prediction = 0.00012347461597528309\n",
      "Iteration 130: Loss = 0.0001229719491675496, Prediction = 0.00012296889326535165\n",
      "Iteration 140: Loss = 0.00012249505380168557, Prediction = 0.00012249725114088506\n",
      "Iteration 150: Loss = 0.00012201815115986392, Prediction = 0.00012203371443320066\n",
      "Iteration 160: Loss = 0.00012160086771473289, Prediction = 0.00012157028686488047\n",
      "Iteration 170: Loss = 0.00012112397234886885, Prediction = 0.00012110330135328695\n",
      "Iteration 180: Loss = 0.00012064707698300481, Prediction = 0.00012063331814715639\n",
      "Iteration 190: Loss = 0.00012017018161714077, Prediction = 0.00012016077380394563\n",
      "Iteration 200: Loss = 0.00011969328625127673, Prediction = 0.00011967912723775953\n",
      "Iteration 210: Loss = 0.00011921639816137031, Prediction = 0.00011919281678274274\n",
      "Iteration 220: Loss = 0.00011867989087477326, Prediction = 0.00011870259186252952\n",
      "Iteration 230: Loss = 0.00011820299550890923, Prediction = 0.0001182083142339252\n",
      "Iteration 240: Loss = 0.00011772610014304519, Prediction = 0.00011770373384933919\n",
      "Iteration 250: Loss = 0.00011718960013240576, Prediction = 0.00011718979658326134\n",
      "Iteration 260: Loss = 0.00011665309284580871, Prediction = 0.00011666586942737922\n",
      "Iteration 270: Loss = 0.00011611659283516929, Prediction = 0.00011612611706368625\n",
      "Iteration 280: Loss = 0.00011558008554857224, Prediction = 0.0001155733407358639\n",
      "Iteration 290: Loss = 0.0001150435782619752, Prediction = 0.00011501022527227178\n",
      "Iteration 300: Loss = 0.00011444746633060277, Prediction = 0.00011443905532360077\n",
      "Iteration 310: Loss = 0.00011385135439923033, Prediction = 0.00011385727702872828\n",
      "Iteration 320: Loss = 0.00011325523519190028, Prediction = 0.00011326547974022105\n",
      "Iteration 330: Loss = 0.00011265912326052785, Prediction = 0.0001126652568927966\n",
      "Iteration 340: Loss = 0.00011206301132915542, Prediction = 0.00011205894406884909\n",
      "Iteration 350: Loss = 0.00011146689939778298, Prediction = 0.00011144672316731885\n",
      "Iteration 360: Loss = 0.00011081117554567754, Prediction = 0.00011083004210377112\n",
      "Iteration 370: Loss = 0.00011021506361430511, Prediction = 0.00011021256068488583\n",
      "Iteration 380: Loss = 0.00010961895168293267, Prediction = 0.00010959633800666779\n",
      "Iteration 390: Loss = 0.00010896322783082724, Prediction = 0.00010897647734964266\n",
      "Iteration 400: Loss = 0.0001083671158994548, Prediction = 0.00010835248394869268\n",
      "Iteration 410: Loss = 0.00010771139204734936, Prediction = 0.00010772815585369244\n",
      "Iteration 420: Loss = 0.00010711528011597693, Prediction = 0.0001071005899575539\n",
      "Iteration 430: Loss = 0.0001064595635398291, Prediction = 0.00010647211456671357\n",
      "Iteration 440: Loss = 0.00010586345160845667, Prediction = 0.00010584286792436615\n",
      "Iteration 450: Loss = 0.00010520773503230885, Prediction = 0.00010521404328756034\n",
      "Iteration 460: Loss = 0.00010461162310093641, Prediction = 0.00010458596079843119\n",
      "Iteration 470: Loss = 0.00010395590652478859, Prediction = 0.00010396023571956903\n",
      "Iteration 480: Loss = 0.00010335979459341615, Prediction = 0.00010333656973671168\n",
      "Iteration 490: Loss = 0.00010270407801726833, Prediction = 0.00010271302744513378\n",
      "Knotting status:  {'0_1': 0.97}\n",
      "Prediction of original input = tensor([[1.0000]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "Original label: 1\n",
      "Prediction of new input = tensor([[0.0001]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n",
      "Target label: tensor([[0.]], device='cuda:0')\n",
      "\n",
      "\n",
      " ---- Attempt 2 ----- \n",
      "\n",
      "Knotting status:  {'3_1': 0.73, '0_1': 0.27}\n",
      "Iteration 0: Loss = 10.255410194396973, Prediction = 0.9999648332595825\n",
      "Iteration 10: Loss = 5.143432140350342, Prediction = 0.994162380695343\n",
      "Iteration 20: Loss = 0.20400017499923706, Prediction = 0.18453776836395264\n",
      "Iteration 30: Loss = 0.0023047043941915035, Prediction = 0.002302024979144335\n",
      "Iteration 40: Loss = 0.0003394464438315481, Prediction = 0.000339409161824733\n",
      "Iteration 50: Loss = 0.00015963398618623614, Prediction = 0.00015960115706548095\n",
      "Iteration 60: Loss = 0.00011921639816137031, Prediction = 0.00011922169505851343\n",
      "Iteration 70: Loss = 0.00010651917546056211, Prediction = 0.000106489060271997\n",
      "Iteration 80: Loss = 0.0001019291375996545, Prediction = 0.00010189659951720387\n",
      "Iteration 90: Loss = 0.00010008120443671942, Prediction = 0.00010006514639826491\n",
      "Iteration 100: Loss = 9.924665937433019e-05, Prediction = 9.921988385031e-05\n",
      "Iteration 110: Loss = 9.871016663964838e-05, Prediction = 9.873136878013611e-05\n",
      "Iteration 120: Loss = 9.835250239120796e-05, Prediction = 9.835858509177342e-05\n",
      "Iteration 130: Loss = 9.799483814276755e-05, Prediction = 9.800310363061726e-05\n",
      "Iteration 140: Loss = 9.763717389432713e-05, Prediction = 9.765327558852732e-05\n",
      "Iteration 150: Loss = 9.727950964588672e-05, Prediction = 9.730191231938079e-05\n",
      "Iteration 160: Loss = 9.692185267340392e-05, Prediction = 9.694681648397818e-05\n",
      "Iteration 170: Loss = 9.65641884249635e-05, Prediction = 9.658758790465072e-05\n",
      "Iteration 180: Loss = 9.620652417652309e-05, Prediction = 9.622528887121007e-05\n",
      "Iteration 190: Loss = 9.584885992808267e-05, Prediction = 9.585876978235319e-05\n",
      "Iteration 200: Loss = 9.549120295559987e-05, Prediction = 9.548964590067044e-05\n",
      "Iteration 210: Loss = 9.513353870715946e-05, Prediction = 9.51192996581085e-05\n",
      "Iteration 220: Loss = 9.477587445871904e-05, Prediction = 9.474679245613515e-05\n",
      "Iteration 230: Loss = 9.435860556550324e-05, Prediction = 9.43702325457707e-05\n",
      "Iteration 240: Loss = 9.400094131706282e-05, Prediction = 9.398909605806693e-05\n",
      "Iteration 250: Loss = 9.358367242384702e-05, Prediction = 9.360386320622638e-05\n",
      "Iteration 260: Loss = 9.32260081754066e-05, Prediction = 9.32169277803041e-05\n",
      "Iteration 270: Loss = 9.28087392821908e-05, Prediction = 9.282742394134402e-05\n",
      "Iteration 280: Loss = 9.245107503375039e-05, Prediction = 9.243223757948726e-05\n",
      "Iteration 290: Loss = 9.203380614053458e-05, Prediction = 9.202767250826582e-05\n",
      "Iteration 300: Loss = 9.161653724731877e-05, Prediction = 9.161649359157309e-05\n",
      "Iteration 310: Loss = 9.119926835410297e-05, Prediction = 9.119618334807456e-05\n",
      "Iteration 320: Loss = 9.078199218492955e-05, Prediction = 9.076733113033697e-05\n",
      "Iteration 330: Loss = 9.036472329171374e-05, Prediction = 9.033128299051896e-05\n",
      "Iteration 340: Loss = 8.988784247776493e-05, Prediction = 8.988652552943677e-05\n",
      "Iteration 350: Loss = 8.941096166381612e-05, Prediction = 8.943040302256122e-05\n",
      "Iteration 360: Loss = 8.899369277060032e-05, Prediction = 8.897081715986133e-05\n",
      "Iteration 370: Loss = 8.851681195665151e-05, Prediction = 8.850895392242819e-05\n",
      "Iteration 380: Loss = 8.803993841866031e-05, Prediction = 8.804302342468873e-05\n",
      "Iteration 390: Loss = 8.75630576047115e-05, Prediction = 8.757520845392719e-05\n",
      "Iteration 400: Loss = 8.708617679076269e-05, Prediction = 8.710446854820475e-05\n",
      "Iteration 410: Loss = 8.660929597681388e-05, Prediction = 8.663312473800033e-05\n",
      "Iteration 420: Loss = 8.619203435955569e-05, Prediction = 8.616087870905176e-05\n",
      "Iteration 430: Loss = 8.571515354560688e-05, Prediction = 8.568899647798389e-05\n",
      "Iteration 440: Loss = 8.523827273165807e-05, Prediction = 8.521703421138227e-05\n",
      "Iteration 450: Loss = 8.476139919366688e-05, Prediction = 8.474579954054207e-05\n",
      "Iteration 460: Loss = 8.428451837971807e-05, Prediction = 8.427396096521989e-05\n",
      "Iteration 470: Loss = 8.380764484172687e-05, Prediction = 8.380162762477994e-05\n",
      "Iteration 480: Loss = 8.333076402777806e-05, Prediction = 8.332987636094913e-05\n",
      "Iteration 490: Loss = 8.285389048978686e-05, Prediction = 8.285856893053278e-05\n",
      "Knotting status:  {'0_1': 0.22, '3_1': 0.76}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    print(f\"\\n\\n ---- Attempt {i} ----- \\n\")\n",
    "    \n",
    "    length = test_features['lengths'][i]\n",
    "    sample = torch.clone(test_features['angles'][i]).unsqueeze(0).to(DEVICE)\n",
    "    orig_prediction = simple_model(sample)\n",
    "    \n",
    "    inner_to_structure(test_features['angles'][i], length, f\"original_{i}\" )\n",
    "    \n",
    "    sample.requires_grad = True\n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = optim.Adam([sample], lr=FLIPPING_LR)\n",
    "    \n",
    "    target = torch.tensor([[1.0 - test_labels[i]]], device=DEVICE)\n",
    "        \n",
    "    new_sample = guide_structure(sample, target, verbose=True)\n",
    "    \n",
    "    res = inner_to_structure(new_sample.detach().to('cpu')[0], length, i)\n",
    "    \n",
    "    if (res and (test_labels[i] == 0)) or (not res and (test_labels[i] == 1)):\n",
    "        if res:\n",
    "            print(\"\\n----ATTENTION----\\n\")\n",
    "        print(f\"Prediction of original input = {orig_prediction}\")\n",
    "        print(f\"Original label: {test_labels[i]}\")\n",
    "        print(f\"Prediction of new input = {simple_model(new_sample)}\")\n",
    "        print(f\"Target label: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec62091-0fe2-49ab-afa1-83af07e90982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:foldingdiff]",
   "language": "python",
   "name": "conda-env-foldingdiff-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
